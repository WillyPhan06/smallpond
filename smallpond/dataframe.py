from __future__ import annotations

import hashlib
import os
import pickle
import time
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from threading import Lock
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import pandas as pd
import pyarrow as arrow
import ray
import ray.exceptions
from loguru import logger

from smallpond.execution.task import Task
from smallpond.io.filesystem import remove_path
from smallpond.logical.dataset import *
from smallpond.logical.node import *
from smallpond.logical.optimizer import Optimizer
from smallpond.logical.planner import Planner
from smallpond.session import SessionBase


class DataFrameCache:
    """
    A cache for storing computed DataFrame results.

    This cache stores the results of _compute() calls, keyed by a hash of the
    DataFrame's optimized logical plan. When the same DataFrame operations are
    executed multiple times, the cached results are returned instead of
    recomputing.

    The cache is thread-safe and can be shared across multiple DataFrames
    within the same session.
    """

    def __init__(self, enabled: bool = True, max_entries: Optional[int] = None):
        """
        Initialize the DataFrame cache.

        Parameters
        ----------
        enabled : bool, default True
            Whether caching is enabled by default.
        max_entries : int, optional
            Maximum number of cache entries. If None, no limit is applied.
            When the limit is reached, the oldest entries are evicted.
        """
        self._cache: Dict[str, Tuple[List[DataSet], datetime]] = {}
        self._lock = Lock()
        self._enabled = enabled
        self._max_entries = max_entries
        self._hits = 0
        self._misses = 0

    @property
    def enabled(self) -> bool:
        """Whether caching is currently enabled."""
        return self._enabled

    @enabled.setter
    def enabled(self, value: bool):
        """Enable or disable caching."""
        self._enabled = value

    def _generate_cache_key(self, plan: Node) -> str:
        """
        Generate a unique cache key based on the logical plan.

        The key is generated by traversing the plan tree and hashing
        the relevant attributes of each node that affect computation results.
        """
        def serialize_value(value: Any) -> Any:
            """Serialize a value to a hashable representation."""
            if value is None:
                return None
            elif isinstance(value, (str, int, float, bool)):
                return value
            elif isinstance(value, (list, tuple)):
                return [serialize_value(v) for v in value]
            elif isinstance(value, dict):
                return {k: serialize_value(v) for k, v in sorted(value.items())}
            elif isinstance(value, set):
                return sorted(list(value))
            elif hasattr(value, "__dict__"):
                # For objects, serialize their __dict__ but skip private attrs and methods
                return {
                    k: serialize_value(v)
                    for k, v in value.__dict__.items()
                    if not k.startswith("_") and not callable(v)
                }
            else:
                # For other types, use string representation
                return str(value)

        def node_to_dict(node: Node) -> dict:
            """Convert a node to a dictionary representation for hashing."""
            result = {
                "type": node.__class__.__name__,
                "id": int(node.id),
            }

            # Collect all public attributes that are not methods or private
            # This ensures any new node types are automatically handled
            for attr_name in dir(node):
                # Skip private attributes, methods, and known non-deterministic attrs
                if attr_name.startswith("_"):
                    continue
                if attr_name in ("input_deps", "ctx", "generated_tasks", "perf_stats",
                                 "perf_metrics", "location", "optimized_plan"):
                    continue

                try:
                    attr_value = getattr(node, attr_name)
                    # Skip methods and callables
                    if callable(attr_value):
                        continue
                    # Serialize the value
                    result[attr_name] = serialize_value(attr_value)
                except Exception as e:
                    # If we can't get/serialize an attribute, log it and skip
                    logger.debug(
                        f"Cache key generation: skipping attribute '{attr_name}' on node "
                        f"{node.__class__.__name__} (id={node.id}) due to serialization error: {e}"
                    )

            # Recursively process input dependencies
            result["input_deps"] = [node_to_dict(dep) for dep in node.input_deps]

            return result

        plan_dict = node_to_dict(plan)
        plan_str = pickle.dumps(plan_dict)
        return hashlib.sha256(plan_str).hexdigest()

    def get(self, plan: Node) -> Optional[List[DataSet]]:
        """
        Get cached results for the given plan.

        Parameters
        ----------
        plan : Node
            The optimized logical plan.

        Returns
        -------
        Optional[List[DataSet]]
            The cached results if available, None otherwise.
        """
        if not self._enabled:
            return None

        key = self._generate_cache_key(plan)
        with self._lock:
            if key in self._cache:
                self._hits += 1
                result, _ = self._cache[key]
                logger.debug(f"Cache hit for plan {plan!r}, key={key[:8]}...")
                return result
            self._misses += 1
            return None

    def put(self, plan: Node, result: List[DataSet]) -> None:
        """
        Store results in the cache.

        Parameters
        ----------
        plan : Node
            The optimized logical plan.
        result : List[DataSet]
            The computed results to cache.
        """
        if not self._enabled:
            return

        key = self._generate_cache_key(plan)
        with self._lock:
            # Evict oldest entries if max_entries is set and reached
            if self._max_entries is not None and len(self._cache) >= self._max_entries:
                # Find and remove the oldest entry
                oldest_key = min(self._cache.keys(), key=lambda k: self._cache[k][1])
                del self._cache[oldest_key]
                logger.debug(f"Evicted cache entry {oldest_key[:8]}...")

            self._cache[key] = (result, datetime.now())
            logger.debug(f"Cached result for plan {plan!r}, key={key[:8]}...")

    def invalidate(self, plan: Optional[Node] = None) -> int:
        """
        Invalidate cache entries.

        Parameters
        ----------
        plan : Node, optional
            If provided, only invalidate the cache entry for this specific plan.
            If None, clear the entire cache.

        Returns
        -------
        int
            The number of entries removed.
        """
        with self._lock:
            if plan is None:
                count = len(self._cache)
                self._cache.clear()
                logger.info(f"Cleared all {count} cache entries")
                return count
            else:
                key = self._generate_cache_key(plan)
                if key in self._cache:
                    del self._cache[key]
                    logger.debug(f"Invalidated cache entry for plan {plan!r}")
                    return 1
                return 0

    def get_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics.

        Returns
        -------
        Dict[str, Any]
            A dictionary containing:
            - entries: Number of cached entries
            - hits: Number of cache hits
            - misses: Number of cache misses
            - hit_rate: Cache hit rate (0.0 to 1.0)
            - enabled: Whether caching is enabled
        """
        with self._lock:
            total = self._hits + self._misses
            return {
                "entries": len(self._cache),
                "hits": self._hits,
                "misses": self._misses,
                "hit_rate": self._hits / total if total > 0 else 0.0,
                "enabled": self._enabled,
            }

    def get_cached_entries(self) -> List[Dict[str, Any]]:
        """
        Get information about all cached entries.

        Returns
        -------
        List[Dict[str, Any]]
            A list of dictionaries, each containing:
            - key: The cache key (truncated)
            - cached_at: When the entry was cached
            - num_datasets: Number of datasets in the cached result
        """
        with self._lock:
            entries = []
            for key, (result, cached_at) in self._cache.items():
                entries.append({
                    "key": key[:16] + "...",
                    "cached_at": cached_at.isoformat(),
                    "num_datasets": len(result),
                })
            return entries

    def reset_stats(self) -> None:
        """Reset cache hit/miss statistics."""
        with self._lock:
            self._hits = 0
            self._misses = 0


# Global default cache instance
_default_cache: Optional[DataFrameCache] = None
_default_cache_lock = Lock()


def get_default_cache() -> DataFrameCache:
    """Get or create the default global cache instance."""
    global _default_cache
    with _default_cache_lock:
        if _default_cache is None:
            _default_cache = DataFrameCache()
        return _default_cache


def set_default_cache(cache: Optional[DataFrameCache]) -> None:
    """Set the default global cache instance."""
    global _default_cache
    with _default_cache_lock:
        _default_cache = cache


class Session(SessionBase):
    # Extended session class with additional methods to create DataFrames.

    def __init__(self, cache: Optional[DataFrameCache] = None, **kwargs):
        """
        Initialize a Session.

        Parameters
        ----------
        cache : DataFrameCache, optional
            A cache instance for storing computed DataFrame results.
            If None, uses the global default cache.
            Pass DataFrameCache(enabled=False) to disable caching.
        **kwargs
            Additional arguments passed to SessionBase.
        """
        super().__init__(**kwargs)
        self._nodes: List[Node] = []

        self._node_to_tasks: Dict[Node, List[Task]] = {}
        """
        When a DataFrame is evaluated, the tasks of the logical plan are stored here.
        Subsequent DataFrames can reuse the tasks to avoid recomputation.
        """

        self._cache = cache if cache is not None else get_default_cache()
        """
        Cache for storing computed DataFrame results.
        """

    @property
    def cache(self) -> DataFrameCache:
        """Get the cache instance for this session."""
        return self._cache

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics for this session.

        Returns
        -------
        Dict[str, Any]
            Cache statistics including entries, hits, misses, and hit rate.
        """
        return self._cache.get_stats()

    def get_cached_entries(self) -> List[Dict[str, Any]]:
        """
        Get information about all cached entries.

        Returns
        -------
        List[Dict[str, Any]]
            Information about each cached entry.
        """
        return self._cache.get_cached_entries()

    def clear_cache(self) -> int:
        """
        Clear all cached DataFrame results.

        Returns
        -------
        int
            The number of cache entries cleared.
        """
        return self._cache.invalidate()

    def set_cache_enabled(self, enabled: bool) -> None:
        """
        Enable or disable caching for this session.

        Parameters
        ----------
        enabled : bool
            Whether to enable caching.
        """
        self._cache.enabled = enabled

    def read_csv(self, paths: Union[str, List[str]], schema: Dict[str, str], delim=",") -> DataFrame:
        """
        Create a DataFrame from CSV files.
        """
        dataset = CsvDataSet(paths, OrderedDict(schema), delim)
        plan = DataSourceNode(self._ctx, dataset)
        return DataFrame(self, plan)

    def read_parquet(
        self,
        paths: Union[str, List[str]],
        recursive: bool = False,
        columns: Optional[List[str]] = None,
        union_by_name: bool = False,
    ) -> DataFrame:
        """
        Create a DataFrame from Parquet files.
        """
        dataset = ParquetDataSet(paths, columns=columns, union_by_name=union_by_name, recursive=recursive)
        plan = DataSourceNode(self._ctx, dataset)
        return DataFrame(self, plan)

    def read_json(self, paths: Union[str, List[str]], schema: Dict[str, str]) -> DataFrame:
        """
        Create a DataFrame from JSON files.
        """
        dataset = JsonDataSet(paths, schema)
        plan = DataSourceNode(self._ctx, dataset)
        return DataFrame(self, plan)

    def from_items(self, items: List[Any]) -> DataFrame:
        """
        Create a DataFrame from a list of local Python objects.
        """

        assert isinstance(items, list), "items must be a list"
        assert len(items) > 0, "items must not be empty"
        if isinstance(items[0], dict):
            return self.from_arrow(arrow.Table.from_pylist(items))
        else:
            return self.from_arrow(arrow.table({"item": items}))

    def from_pandas(self, df: pd.DataFrame) -> DataFrame:
        """
        Create a DataFrame from a pandas DataFrame.
        """
        plan = DataSourceNode(self._ctx, PandasDataSet(df))
        return DataFrame(self, plan)

    def from_arrow(self, table: arrow.Table) -> DataFrame:
        """
        Create a DataFrame from a pyarrow Table.
        """
        plan = DataSourceNode(self._ctx, ArrowTableDataSet(table))
        return DataFrame(self, plan)

    def partial_sql(self, query: str, *inputs: DataFrame, **kwargs) -> DataFrame:
        """
        Execute a SQL query on each partition of the input DataFrames.

        The query can contain placeholder `{0}`, `{1}`, etc. for the input DataFrames.
        If multiple DataFrames are provided, they must have the same number of partitions.

        Examples
        --------
        Join two datasets. You need to make sure the join key is correctly partitioned.

        .. code-block::

            a = sp.read_parquet("a/*.parquet").repartition(10, hash_by="id")
            b = sp.read_parquet("b/*.parquet").repartition(10, hash_by="id")
            c = sp.partial_sql("select * from {0} join {1} on a.id = b.id", a, b)
        """

        plan = SqlEngineNode(self._ctx, tuple(input.plan for input in inputs), query, **kwargs)
        recompute = any(input.need_recompute for input in inputs)
        return DataFrame(self, plan, recompute=recompute)

    def wait(self, *dfs: DataFrame):
        """
        Wait for all DataFrames to be computed.

        Example
        -------
        This can be used to wait for multiple outputs from a pipeline:

        .. code-block::

            df = sp.read_parquet("input/*.parquet")
            output1 = df.write_parquet("output1")
            output2 = df.map("col1, col2").write_parquet("output2")
            sp.wait(output1, output2)
        """
        ray.get([task.run_on_ray() for df in dfs for task in df._get_or_create_tasks()])

    def graph(self) -> Digraph:
        """
        Get the DataFrame graph.
        """
        dot = Digraph(comment="SmallPond")
        for node in self._nodes:
            dot.node(str(node.id), repr(node))
            for dep in node.input_deps:
                dot.edge(str(dep.id), str(node.id))
        return dot

    def shutdown(self):
        """
        Shutdown the session.
        """
        # prevent shutdown from being called multiple times
        if hasattr(self, "_shutdown_called"):
            return
        self._shutdown_called = True

        # log status
        finished = self._all_tasks_finished()
        with open(self._runtime_ctx.job_status_path, "a") as fout:
            status = "success" if finished else "failure"
            fout.write(f"{status}@{datetime.now():%Y-%m-%d-%H-%M-%S}\n")

        # clean up runtime directories if success
        if finished:
            logger.info("all tasks are finished, cleaning up")
            self._runtime_ctx.cleanup(remove_output_root=self.config.remove_output_root)
        else:
            logger.warning("tasks are not finished!")

        super().shutdown()

    def _summarize_task(self) -> Tuple[int, int]:
        """
        Return the total number of tasks and the number of tasks that are finished.
        """
        dataset_refs = [task._dataset_ref for tasks in self._node_to_tasks.values() for task in tasks if task._dataset_ref is not None]
        ready_tasks, _ = ray.wait(dataset_refs, num_returns=len(dataset_refs), timeout=0, fetch_local=False)
        return len(dataset_refs), len(ready_tasks)

    def _all_tasks_finished(self) -> bool:
        """
        Check if all tasks are finished.
        """
        dataset_refs = [task._dataset_ref for tasks in self._node_to_tasks.values() for task in tasks]
        try:
            ray.get(dataset_refs, timeout=0)
        except Exception:
            # GetTimeoutError is raised if any task is not finished
            # RuntimeError is raised if any task failed
            return False
        return True


class DataFrame:
    """
    A distributed data collection. It represents a 2 dimensional table of rows and columns.

    Internally, it's a wrapper around a `Node` and a `Session` required for execution.
    """

    def __init__(self, session: Session, plan: Node, recompute: bool = False, use_cache: bool = True):
        self.session = session
        self.plan = plan
        self.optimized_plan: Optional[Node] = None
        self.need_recompute = recompute
        """Whether to recompute the data regardless of whether it's already computed."""
        self._use_cache = use_cache
        """Whether to use caching for this DataFrame's computations."""
        self._compute_lock = Lock()
        """Lock to ensure thread-safe access to optimized_plan during _compute()."""

        session._nodes.append(plan)

    def __str__(self) -> str:
        return repr(self.plan)

    def _get_or_create_tasks(self) -> List[Task]:
        """
        Get or create tasks to compute the data.
        """
        # optimize the plan
        if self.optimized_plan is None:
            logger.info(f"optimizing\n{LogicalPlan(self.session._ctx, self.plan)}")
            self.optimized_plan = Optimizer(exclude_nodes=set(self.session._node_to_tasks.keys())).visit(self.plan)
            logger.info(f"optimized\n{LogicalPlan(self.session._ctx, self.optimized_plan)}")
        # return the tasks if already created
        if tasks := self.session._node_to_tasks.get(self.optimized_plan):
            return tasks

        # remove all completed task files if recompute is needed
        if self.need_recompute:
            remove_path(
                os.path.join(
                    self.session._runtime_ctx.completed_task_dir,
                    str(self.optimized_plan.id),
                )
            )
            logger.info(f"cleared all results of {self.optimized_plan!r}")

        # create tasks for the optimized plan
        planner = Planner(self.session._runtime_ctx)
        # let planner update self.session._node_to_tasks
        planner.node_to_tasks = self.session._node_to_tasks
        return planner.visit(self.optimized_plan)

    def is_computed(self) -> bool:
        """
        Check if the data is ready on disk.
        """
        if tasks := self.session._node_to_tasks.get(self.plan):
            _, unready_tasks = ray.wait(tasks, timeout=0)
            return len(unready_tasks) == 0
        return False

    def compute(self) -> None:
        """
        Compute the data.

        This operation will trigger execution of the lazy transformations performed on this DataFrame.
        """
        self._compute()

    def _compute(self, use_cache: Optional[bool] = None) -> List[DataSet]:
        """
        Compute the data and return the datasets.

        This method is thread-safe. Multiple threads calling _compute() on the same
        DataFrame will be serialized to avoid race conditions when optimizing the plan.

        Parameters
        ----------
        use_cache : bool, optional
            Whether to use caching for this computation.
            If None, uses the DataFrame's default cache setting.
            If True, will check the cache first and store results in cache.
            If False, will bypass the cache entirely.
        """
        # Determine if we should use cache
        should_use_cache = use_cache if use_cache is not None else self._use_cache

        # Use lock to ensure thread-safe access to optimized_plan
        with self._compute_lock:
            # Ensure the plan is optimized before checking cache
            if self.optimized_plan is None:
                # This will be set by _get_or_create_tasks, but we need it for cache key
                logger.info(f"optimizing\n{LogicalPlan(self.session._ctx, self.plan)}")
                self.optimized_plan = Optimizer(exclude_nodes=set(self.session._node_to_tasks.keys())).visit(self.plan)
                logger.info(f"optimized\n{LogicalPlan(self.session._ctx, self.optimized_plan)}")

            # Check cache first (unless recompute is needed or cache is disabled)
            if should_use_cache and not self.need_recompute:
                cached_result = self.session._cache.get(self.optimized_plan)
                if cached_result is not None:
                    return cached_result

            # Compute the data
            for retry_count in range(3):
                try:
                    result = ray.get([task.run_on_ray() for task in self._get_or_create_tasks()])

                    # Store in cache if caching is enabled
                    if should_use_cache:
                        self.session._cache.put(self.optimized_plan, result)

                    return result
                except ray.exceptions.RuntimeEnvSetupError as e:
                    # XXX: Ray may raise this error when a worker is interrupted.
                    #      ```
                    #      ray.exceptions.RuntimeEnvSetupError: Failed to set up runtime environment.
                    #      Failed to create runtime env for job 01000000, status = IOError:
                    #      on_read bad version, maybe there are some network problems, will fail the request.
                    #      ```
                    #      This is a bug of Ray and has been fixed in Ray 2.24: <https://github.com/ray-project/ray/pull/45513>
                    #      However, since Ray dropped support for Python 3.8 since 2.11, we can not upgrade Ray.
                    #      So we catch this error and retry by ourselves.
                    logger.error(f"found ray RuntimeEnvSetupError, retrying...\n{e}")
                    time.sleep(10 << retry_count)
            raise RuntimeError("Failed to compute data after 3 retries")

    # operations

    def recompute(self) -> DataFrame:
        """
        Always recompute the data regardless of whether it's already computed.

        This method also clears any cached result for this DataFrame to free memory,
        since the cached result will no longer be valid after recomputation.

        Examples
        --------
        Modify the code as follows and rerun:

        .. code-block:: diff

            - df = input.select('a')
            + df = input.select('b').recompute()

        The result of `input` can be reused.
        """
        # Clear the cached result if it exists, since we're going to recompute
        if self.optimized_plan is not None:
            cleared = self.session._cache.invalidate(self.optimized_plan)
            if cleared > 0:
                logger.debug(f"Cleared cached result for {self.optimized_plan!r} due to recompute()")

        self.need_recompute = True
        return self

    def no_cache(self) -> DataFrame:
        """
        Disable caching for this DataFrame's computations.

        When caching is disabled, results will always be computed fresh and
        will not be stored in or retrieved from the cache.

        Returns
        -------
        DataFrame
            Returns self for method chaining.

        Examples
        --------
        .. code-block::

            # Disable caching for a specific computation
            result = df.filter('x > 10').no_cache().count()

            # Chain with other operations
            df.map('a, b').no_cache().to_pandas()
        """
        self._use_cache = False
        return self

    def clear_cache(self) -> int:
        """
        Clear the cached result for this specific DataFrame.

        Returns
        -------
        int
            The number of cache entries removed (0 or 1).

        Examples
        --------
        .. code-block::

            df = sp.read_parquet("data/*.parquet").filter('x > 10')
            count1 = df.count()  # Computes and caches
            df.clear_cache()     # Clears cached result for this df
            count2 = df.count()  # Computes again
        """
        if self.optimized_plan is not None:
            return self.session._cache.invalidate(self.optimized_plan)
        return 0

    def repartition(
        self,
        npartitions: int,
        hash_by: Union[str, List[str], None] = None,
        by: Optional[str] = None,
        by_rows: bool = False,
        **kwargs,
    ) -> DataFrame:
        """
        Repartition the data into the given number of partitions.

        Parameters
        ----------
        npartitions
            The dataset would be split and distributed to `npartitions` partitions.
            If not specified, the number of partitions would be the default partition size of the context.
        hash_by, optional
            If specified, the dataset would be repartitioned by the hash of the given columns.
        by, optional
            If specified, the dataset would be repartitioned by the given column.
        by_rows, optional
            If specified, the dataset would be repartitioned by rows instead of by files.

        Examples
        --------
        .. code-block::

            df = df.repartition(10)                 # evenly distributed
            df = df.repartition(10, by_rows=True)   # evenly distributed by rows
            df = df.repartition(10, hash_by='host') # hash partitioned
            df = df.repartition(10, by='bucket')    # partitioned by column
        """
        if by is not None:
            assert hash_by is None, "cannot specify both by and hash_by"
            plan = ShuffleNode(
                self.session._ctx,
                (self.plan,),
                npartitions,
                data_partition_column=by,
                **kwargs,
            )
        elif hash_by is not None:
            hash_columns = [hash_by] if isinstance(hash_by, str) else hash_by
            plan = HashPartitionNode(self.session._ctx, (self.plan,), npartitions, hash_columns, **kwargs)
        else:
            plan = EvenlyDistributedPartitionNode(
                self.session._ctx,
                (self.plan,),
                npartitions,
                partition_by_rows=by_rows,
                **kwargs,
            )
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def random_shuffle(self, **kwargs) -> DataFrame:
        """
        Randomly shuffle all rows globally.
        """

        repartition = HashPartitionNode(
            self.session._ctx,
            (self.plan,),
            self.plan.num_partitions,
            random_shuffle=True,
            **kwargs,
        )
        plan = SqlEngineNode(
            self.session._ctx,
            (repartition,),
            r"select * from {0} order by random()",
            **kwargs,
        )
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def partial_sort(self, by: Union[str, List[str]], **kwargs) -> DataFrame:
        """
        Sort rows by the given columns in each partition.

        Parameters
        ----------
        by
            A column or a list of columns to sort by.

        Examples
        --------
        .. code-block::

            df = df.partial_sort(by='a')
            df = df.partial_sort(by=['a', 'b desc'])
        """

        by = [by] if isinstance(by, str) else by
        plan = SqlEngineNode(
            self.session._ctx,
            (self.plan,),
            f"select * from {{0}} order by {', '.join(by)}",
            **kwargs,
        )
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def filter(self, sql_or_func: Union[str, Callable[[Dict[str, Any]], bool]], **kwargs) -> DataFrame:
        """
        Filter out rows that don't satisfy the given predicate.

        Parameters
        ----------
        sql_or_func
            A SQL expression or a predicate function.
            For functions, it should take a dictionary of columns as input and returns a boolean.
            SQL expression is preferred as it's more efficient.

        Examples
        --------
        .. code-block::

            df = df.filter('a > 1')
            df = df.filter(lambda r: r['a'] > 1)
        """
        if isinstance(sql := sql_or_func, str):
            plan = SqlEngineNode(
                self.session._ctx,
                (self.plan,),
                f"select * from {{0}} where ({sql})",
                **kwargs,
            )
        elif isinstance(func := sql_or_func, Callable):

            def process_func(_runtime_ctx, tables: List[arrow.Table]) -> arrow.Table:
                table = tables[0]
                return table.filter([func(row) for row in table.to_pylist()])

            plan = ArrowBatchNode(self.session._ctx, (self.plan,), process_func=process_func, **kwargs)
        else:
            raise ValueError("condition must be a SQL expression or a predicate function")
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def map(
        self,
        sql_or_func: Union[str, Callable[[Dict[str, Any]], Dict[str, Any]]],
        *,
        schema: Optional[arrow.Schema] = None,
        **kwargs,
    ) -> DataFrame:
        """
        Apply a function to each row.

        Parameters
        ----------
        sql_or_func
            A SQL expression or a function to apply to each row.
            For functions, it should take a dictionary of columns as input and returns a dictionary of columns.
            SQL expression is preferred as it's more efficient.
        schema, optional
            The schema of the output DataFrame.
            If not passed, will be inferred from the first row of the mapping values.
        udfs, optional
            A list of user defined functions to be referenced in the SQL expression.

        Examples
        --------
        .. code-block::

            df = df.map('a, b')
            df = df.map('a + b as c')
            df = df.map(lambda row: {'c': row['a'] + row['b']})


        Use user-defined functions in SQL expression:

        .. code-block::

            @udf(params=[UDFType.INT, UDFType.INT], return_type=UDFType.INT)
            def gcd(a: int, b: int) -> int:
                while b:
                    a, b = b, a % b
                return a
            # load python udf
            df = df.map('gcd(a, b)', udfs=[gcd])

            # load udf from duckdb extension
            df = df.map('gcd(a, b)', udfs=['path/to/udf.duckdb_extension'])

        """
        if isinstance(sql := sql_or_func, str):
            plan = SqlEngineNode(self.session._ctx, (self.plan,), f"select {sql} from {{0}}", **kwargs)
        elif isinstance(func := sql_or_func, Callable):

            def process_func(_runtime_ctx, tables: List[arrow.Table]) -> arrow.Table:
                output_rows = [func(row) for row in tables[0].to_pylist()]
                return arrow.Table.from_pylist(output_rows, schema=schema)

            plan = ArrowBatchNode(self.session._ctx, (self.plan,), process_func=process_func, **kwargs)
        else:
            raise ValueError(f"must be a SQL expression or a function: {sql_or_func!r}")
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def flat_map(
        self,
        sql_or_func: Union[str, Callable[[Dict[str, Any]], List[Dict[str, Any]]]],
        *,
        schema: Optional[arrow.Schema] = None,
        **kwargs,
    ) -> DataFrame:
        """
        Apply a function to each row and flatten the result.

        Parameters
        ----------
        sql_or_func
            A SQL expression or a function to apply to each row.
            For functions, it should take a dictionary of columns as input and returns a list of dictionaries.
            SQL expression is preferred as it's more efficient.
        schema, optional
            The schema of the output DataFrame.
            If not passed, will be inferred from the first row of the mapping values.

        Examples
        --------
        .. code-block::

            df = df.flat_map('unnest(array[a, b]) as c')
            df = df.flat_map(lambda row: [{'c': row['a']}, {'c': row['b']}])
        """
        if isinstance(sql := sql_or_func, str):

            plan = SqlEngineNode(self.session._ctx, (self.plan,), f"select {sql} from {{0}}", **kwargs)
        elif isinstance(func := sql_or_func, Callable):

            def process_func(_runtime_ctx, tables: List[arrow.Table]) -> arrow.Table:
                output_rows = [item for row in tables[0].to_pylist() for item in func(row)]
                return arrow.Table.from_pylist(output_rows, schema=schema)

            plan = ArrowBatchNode(self.session._ctx, (self.plan,), process_func=process_func, **kwargs)
        else:
            raise ValueError(f"must be a SQL expression or a function: {sql_or_func!r}")
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def map_batches(
        self,
        func: Callable[[arrow.Table], arrow.Table],
        *,
        batch_size: int = 122880,
        **kwargs,
    ) -> DataFrame:
        """
        Apply the given function to batches of data.

        Parameters
        ----------
        func
            A function or a callable class to apply to each batch of data.
            It should take a `arrow.Table` as input and returns a `arrow.Table`.
        batch_size, optional
            The number of rows in each batch. Defaults to 122880.
        """

        def process_func(_runtime_ctx, tables: List[arrow.Table]) -> arrow.Table:
            return func(tables[0])

        plan = ArrowBatchNode(
            self.session._ctx,
            (self.plan,),
            process_func=process_func,
            streaming_batch_size=batch_size,
            **kwargs,
        )
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def limit(self, limit: int) -> DataFrame:
        """
        Limit the number of rows to the given number.

        Unlike `take`, this method doesn't trigger execution.
        """
        plan = LimitNode(self.session._ctx, self.plan, limit)
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def sample(
        self,
        n: Optional[int] = None,
        fraction: Optional[float] = None,
        seed: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        Return a random sample of rows from the DataFrame.

        This is useful for quickly inspecting a representative subset of the data
        without having to look at all rows. Users can specify either an exact number
        of rows or a fraction of total rows to sample.

        Parameters
        ----------
        n : int, optional
            The exact number of rows to sample. Must be a positive integer.
            Cannot be used together with `fraction`.
            If `n` is larger than the total number of rows in the DataFrame,
            all available rows will be returned (no error is raised).
        fraction : float, optional
            The fraction of rows to sample, between 0.0 and 1.0 (exclusive of 0, inclusive of 1).
            For example, 0.1 means 10% of rows. Cannot be used together with `n`.
        seed : int, optional
            Random seed for reproducible sampling. Can be used with either `n` or `fraction`.
            If not specified, sampling will be random each time.

        Returns
        -------
        List[Dict[str, Any]]
            A list of dictionaries, where each dictionary represents a row with column names as keys.

        Raises
        ------
        ValueError
            If neither `n` nor `fraction` is specified, or if both are specified.
            If `n` is not a positive integer.
            If `fraction` is not between 0 and 1.

        Examples
        --------
        Sample exactly 5 random rows:

        .. code-block::

            rows = df.sample(n=5)

        Sample 10% of the data:

        .. code-block::

            rows = df.sample(fraction=0.1)

        Sample with a fixed seed for reproducibility (using n):

        .. code-block::

            rows = df.sample(n=10, seed=42)

        Sample with a fixed seed for reproducibility (using fraction):

        .. code-block::

            rows = df.sample(fraction=0.2, seed=42)

        Notes
        -----
        This operation triggers execution of the lazy transformations performed on this DataFrame.
        For very large datasets, using `fraction` with a small value is more efficient than
        specifying a large `n`, as it can filter rows early in the pipeline.
        """
        # Validate parameters
        if n is None and fraction is None:
            raise ValueError("Must specify either 'n' (number of rows) or 'fraction' (proportion of rows)")
        if n is not None and fraction is not None:
            raise ValueError("Cannot specify both 'n' and 'fraction'. Please choose one.")
        if n is not None:
            if not isinstance(n, int) or n <= 0:
                raise ValueError(f"'n' must be a positive integer, got {n}")
        if fraction is not None:
            if not isinstance(fraction, (int, float)) or fraction <= 0 or fraction > 1:
                raise ValueError(
                    f"'fraction' must be a decimal number between 0 (exclusive) and 1 (inclusive), got {fraction}. "
                    "Examples: 0.1 for 10%, 0.25 for 25%, 0.5 for 50%, 1.0 for 100%."
                )

        # Build sample specification and method based on parameters
        if fraction is not None:
            sample_spec = f"{fraction * 100}%"
            sample_method = "bernoulli"
        else:
            sample_spec = f"{n} ROWS"
            sample_method = "reservoir"

        # Build SQL with optional seed for reproducibility
        seed_clause = f", repeatable({seed})" if seed is not None else ""
        sql = f"SELECT * FROM {{0}} USING SAMPLE {sample_spec} ({sample_method}{seed_clause})"

        plan = SqlEngineNode(self.session._ctx, (self.plan,), sql)
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache).take_all()

    def write_parquet(self, path: str) -> None:
        """
        Write data to a series of parquet files under the given path.

        This is a blocking operation. See :func:`write_parquet_lazy` for a non-blocking version.

        Examples
        --------
        .. code-block::

            df.write_parquet('output')
        """
        self.write_parquet_lazy(path).compute()

    def write_parquet_lazy(self, path: str) -> DataFrame:
        """
        Write data to a series of parquet files under the given path.

        This is a non-blocking operation. See :func:`write_parquet` for a blocking version.

        Examples
        --------
        .. code-block::

            o1 = df.write_parquet_lazy('output1')
            o2 = df.write_parquet_lazy('output2')
            sp.wait(o1, o2)
        """

        plan = DataSinkNode(self.session._ctx, (self.plan,), os.path.abspath(path), type="link_or_copy")
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    # inspection

    def count(self) -> int:
        """
        Count the number of rows.

        If this dataframe consists of more than a read, or if the row count can't be determined from
        the metadata provided by the datasource, then this operation will trigger execution of the
        lazy transformations performed on this dataframe.
        """
        datasets = self._compute()
        # FIXME: don't use ThreadPoolExecutor because duckdb results will be mixed up
        return sum(dataset.num_rows for dataset in datasets)

    def take(self, limit: int) -> List[Dict[str, Any]]:
        """
        Return up to `limit` rows.

        This operation will trigger execution of the lazy transformations performed on this DataFrame.
        """
        if self.is_computed() or isinstance(self.plan, DataSourceNode):
            datasets = self._compute()
        else:
            datasets = self.limit(limit)._compute()
        rows = []
        for dataset in datasets:
            for batch in dataset.to_batch_reader():
                rows.extend(batch.to_pylist())
                if len(rows) >= limit:
                    return rows[:limit]
        return rows

    def take_all(self) -> List[Dict[str, Any]]:
        """
        Return all rows.

        This operation will trigger execution of the lazy transformations performed on this DataFrame.
        """
        datasets = self._compute()
        rows = []
        for dataset in datasets:
            for batch in dataset.to_batch_reader():
                rows.extend(batch.to_pylist())
        return rows

    def to_pandas(self) -> pd.DataFrame:
        """
        Convert to a pandas DataFrame.

        This operation will trigger execution of the lazy transformations performed on this DataFrame.
        """
        datasets = self._compute()
        with ThreadPoolExecutor() as pool:
            return pd.concat(pool.map(lambda dataset: dataset.to_pandas(), datasets))

    def to_arrow(self) -> arrow.Table:
        """
        Convert to an arrow Table.

        This operation will trigger execution of the lazy transformations performed on this DataFrame.
        """
        datasets = self._compute()
        with ThreadPoolExecutor() as pool:
            return arrow.concat_tables(pool.map(lambda dataset: dataset.to_arrow_table(), datasets))
