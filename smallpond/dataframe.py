from __future__ import annotations

import hashlib
import os
import pickle
import time
from collections import OrderedDict
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime
from threading import Lock
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

import pandas as pd
import pyarrow as arrow
import ray
import ray.exceptions
from loguru import logger

from smallpond.execution.task import Task
from smallpond.io.filesystem import remove_path
from smallpond.logical.dataset import *
from smallpond.logical.node import *
from smallpond.logical.optimizer import Optimizer
from smallpond.logical.planner import Planner
from smallpond.session import SessionBase


class DataFrameCache:
    """
    A cache for storing computed DataFrame results.

    This cache stores the results of _compute() calls, keyed by a hash of the
    DataFrame's optimized logical plan. When the same DataFrame operations are
    executed multiple times, the cached results are returned instead of
    recomputing.

    The cache is thread-safe and can be shared across multiple DataFrames
    within the same session.
    """

    def __init__(self, enabled: bool = True, max_entries: Optional[int] = None):
        """
        Initialize the DataFrame cache.

        Parameters
        ----------
        enabled : bool, default True
            Whether caching is enabled by default.
        max_entries : int, optional
            Maximum number of cache entries. If None, no limit is applied.
            When the limit is reached, the oldest entries are evicted.
        """
        self._cache: Dict[str, Tuple[List[DataSet], datetime]] = {}
        self._lock = Lock()
        self._enabled = enabled
        self._max_entries = max_entries
        self._hits = 0
        self._misses = 0

    @property
    def enabled(self) -> bool:
        """Whether caching is currently enabled."""
        return self._enabled

    @enabled.setter
    def enabled(self, value: bool):
        """Enable or disable caching."""
        self._enabled = value

    def _generate_cache_key(self, plan: Node) -> str:
        """
        Generate a unique cache key based on the logical plan.

        The key is generated by traversing the plan tree and hashing
        the relevant attributes of each node that affect computation results.
        """
        def serialize_value(value: Any) -> Any:
            """Serialize a value to a hashable representation."""
            if value is None:
                return None
            elif isinstance(value, (str, int, float, bool)):
                return value
            elif isinstance(value, (list, tuple)):
                return [serialize_value(v) for v in value]
            elif isinstance(value, dict):
                return {k: serialize_value(v) for k, v in sorted(value.items())}
            elif isinstance(value, set):
                return sorted(list(value))
            elif hasattr(value, "__dict__"):
                # For objects, serialize their __dict__ but skip private attrs and methods
                return {
                    k: serialize_value(v)
                    for k, v in value.__dict__.items()
                    if not k.startswith("_") and not callable(v)
                }
            else:
                # For other types, use string representation
                return str(value)

        def node_to_dict(node: Node) -> dict:
            """Convert a node to a dictionary representation for hashing."""
            result = {
                "type": node.__class__.__name__,
                "id": int(node.id),
            }

            # Collect all public attributes that are not methods or private
            # This ensures any new node types are automatically handled
            for attr_name in dir(node):
                # Skip private attributes, methods, and known non-deterministic attrs
                if attr_name.startswith("_"):
                    continue
                if attr_name in ("input_deps", "ctx", "generated_tasks", "perf_stats",
                                 "perf_metrics", "location", "optimized_plan"):
                    continue

                try:
                    attr_value = getattr(node, attr_name)
                    # Skip methods and callables
                    if callable(attr_value):
                        continue
                    # Serialize the value
                    result[attr_name] = serialize_value(attr_value)
                except Exception as e:
                    # If we can't get/serialize an attribute, log it and skip
                    logger.debug(
                        f"Cache key generation: skipping attribute '{attr_name}' on node "
                        f"{node.__class__.__name__} (id={node.id}) due to serialization error: {e}"
                    )

            # Recursively process input dependencies
            result["input_deps"] = [node_to_dict(dep) for dep in node.input_deps]

            return result

        plan_dict = node_to_dict(plan)
        plan_str = pickle.dumps(plan_dict)
        return hashlib.sha256(plan_str).hexdigest()

    def get(self, plan: Node) -> Optional[List[DataSet]]:
        """
        Get cached results for the given plan.

        Parameters
        ----------
        plan : Node
            The optimized logical plan.

        Returns
        -------
        Optional[List[DataSet]]
            The cached results if available, None otherwise.
        """
        if not self._enabled:
            return None

        key = self._generate_cache_key(plan)
        with self._lock:
            if key in self._cache:
                self._hits += 1
                result, _ = self._cache[key]
                logger.debug(f"Cache hit for plan {plan!r}, key={key[:8]}...")
                return result
            self._misses += 1
            return None

    def put(self, plan: Node, result: List[DataSet]) -> None:
        """
        Store results in the cache.

        Parameters
        ----------
        plan : Node
            The optimized logical plan.
        result : List[DataSet]
            The computed results to cache.
        """
        if not self._enabled:
            return

        key = self._generate_cache_key(plan)
        with self._lock:
            # Evict oldest entries if max_entries is set and reached
            if self._max_entries is not None and len(self._cache) >= self._max_entries:
                # Find and remove the oldest entry
                oldest_key = min(self._cache.keys(), key=lambda k: self._cache[k][1])
                del self._cache[oldest_key]
                logger.debug(f"Evicted cache entry {oldest_key[:8]}...")

            self._cache[key] = (result, datetime.now())
            logger.debug(f"Cached result for plan {plan!r}, key={key[:8]}...")

    def invalidate(self, plan: Optional[Node] = None) -> int:
        """
        Invalidate cache entries.

        Parameters
        ----------
        plan : Node, optional
            If provided, only invalidate the cache entry for this specific plan.
            If None, clear the entire cache.

        Returns
        -------
        int
            The number of entries removed.
        """
        with self._lock:
            if plan is None:
                count = len(self._cache)
                self._cache.clear()
                logger.info(f"Cleared all {count} cache entries")
                return count
            else:
                key = self._generate_cache_key(plan)
                if key in self._cache:
                    del self._cache[key]
                    logger.debug(f"Invalidated cache entry for plan {plan!r}")
                    return 1
                return 0

    def get_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics.

        Returns
        -------
        Dict[str, Any]
            A dictionary containing:
            - entries: Number of cached entries
            - hits: Number of cache hits
            - misses: Number of cache misses
            - hit_rate: Cache hit rate (0.0 to 1.0)
            - enabled: Whether caching is enabled
        """
        with self._lock:
            total = self._hits + self._misses
            return {
                "entries": len(self._cache),
                "hits": self._hits,
                "misses": self._misses,
                "hit_rate": self._hits / total if total > 0 else 0.0,
                "enabled": self._enabled,
            }

    def get_cached_entries(self) -> List[Dict[str, Any]]:
        """
        Get information about all cached entries.

        Returns
        -------
        List[Dict[str, Any]]
            A list of dictionaries, each containing:
            - key: The cache key (truncated)
            - cached_at: When the entry was cached
            - num_datasets: Number of datasets in the cached result
        """
        with self._lock:
            entries = []
            for key, (result, cached_at) in self._cache.items():
                entries.append({
                    "key": key[:16] + "...",
                    "cached_at": cached_at.isoformat(),
                    "num_datasets": len(result),
                })
            return entries

    def reset_stats(self) -> None:
        """Reset cache hit/miss statistics."""
        with self._lock:
            self._hits = 0
            self._misses = 0


# Global default cache instance
_default_cache: Optional[DataFrameCache] = None
_default_cache_lock = Lock()


def get_default_cache() -> DataFrameCache:
    """Get or create the default global cache instance."""
    global _default_cache
    with _default_cache_lock:
        if _default_cache is None:
            _default_cache = DataFrameCache()
        return _default_cache


def set_default_cache(cache: Optional[DataFrameCache]) -> None:
    """Set the default global cache instance."""
    global _default_cache
    with _default_cache_lock:
        _default_cache = cache


class Session(SessionBase):
    # Extended session class with additional methods to create DataFrames.

    def __init__(self, cache: Optional[DataFrameCache] = None, **kwargs):
        """
        Initialize a Session.

        Parameters
        ----------
        cache : DataFrameCache, optional
            A cache instance for storing computed DataFrame results.
            If None, uses the global default cache.
            Pass DataFrameCache(enabled=False) to disable caching.
        **kwargs
            Additional arguments passed to SessionBase.
        """
        super().__init__(**kwargs)
        self._nodes: List[Node] = []

        self._node_to_tasks: Dict[Node, List[Task]] = {}
        """
        When a DataFrame is evaluated, the tasks of the logical plan are stored here.
        Subsequent DataFrames can reuse the tasks to avoid recomputation.
        """

        self._cache = cache if cache is not None else get_default_cache()
        """
        Cache for storing computed DataFrame results.
        """

    @property
    def cache(self) -> DataFrameCache:
        """Get the cache instance for this session."""
        return self._cache

    def get_cache_stats(self) -> Dict[str, Any]:
        """
        Get cache statistics for this session.

        Returns
        -------
        Dict[str, Any]
            Cache statistics including entries, hits, misses, and hit rate.
        """
        return self._cache.get_stats()

    def get_cached_entries(self) -> List[Dict[str, Any]]:
        """
        Get information about all cached entries.

        Returns
        -------
        List[Dict[str, Any]]
            Information about each cached entry.
        """
        return self._cache.get_cached_entries()

    def clear_cache(self) -> int:
        """
        Clear all cached DataFrame results.

        Returns
        -------
        int
            The number of cache entries cleared.
        """
        return self._cache.invalidate()

    def set_cache_enabled(self, enabled: bool) -> None:
        """
        Enable or disable caching for this session.

        Parameters
        ----------
        enabled : bool
            Whether to enable caching.
        """
        self._cache.enabled = enabled

    def read_csv(self, paths: Union[str, List[str]], schema: Dict[str, str], delim=",") -> DataFrame:
        """
        Create a DataFrame from CSV files.
        """
        dataset = CsvDataSet(paths, OrderedDict(schema), delim)
        plan = DataSourceNode(self._ctx, dataset)
        return DataFrame(self, plan)

    def read_parquet(
        self,
        paths: Union[str, List[str]],
        recursive: bool = False,
        columns: Optional[List[str]] = None,
        union_by_name: bool = False,
    ) -> DataFrame:
        """
        Create a DataFrame from Parquet files.
        """
        dataset = ParquetDataSet(paths, columns=columns, union_by_name=union_by_name, recursive=recursive)
        plan = DataSourceNode(self._ctx, dataset)
        return DataFrame(self, plan)

    def read_json(self, paths: Union[str, List[str]], schema: Dict[str, str]) -> DataFrame:
        """
        Create a DataFrame from JSON files.
        """
        dataset = JsonDataSet(paths, schema)
        plan = DataSourceNode(self._ctx, dataset)
        return DataFrame(self, plan)

    def from_items(self, items: List[Any]) -> DataFrame:
        """
        Create a DataFrame from a list of local Python objects.
        """

        assert isinstance(items, list), "items must be a list"
        assert len(items) > 0, "items must not be empty"
        if isinstance(items[0], dict):
            return self.from_arrow(arrow.Table.from_pylist(items))
        else:
            return self.from_arrow(arrow.table({"item": items}))

    def from_pandas(self, df: pd.DataFrame) -> DataFrame:
        """
        Create a DataFrame from a pandas DataFrame.
        """
        plan = DataSourceNode(self._ctx, PandasDataSet(df))
        return DataFrame(self, plan)

    def from_arrow(self, table: arrow.Table) -> DataFrame:
        """
        Create a DataFrame from a pyarrow Table.
        """
        plan = DataSourceNode(self._ctx, ArrowTableDataSet(table))
        return DataFrame(self, plan)

    def partial_sql(self, query: str, *inputs: DataFrame, **kwargs) -> DataFrame:
        """
        Execute a SQL query on each partition of the input DataFrames.

        The query can contain placeholder `{0}`, `{1}`, etc. for the input DataFrames.
        If multiple DataFrames are provided, they must have the same number of partitions.

        Examples
        --------
        Join two datasets. You need to make sure the join key is correctly partitioned.

        .. code-block::

            a = sp.read_parquet("a/*.parquet").repartition(10, hash_by="id")
            b = sp.read_parquet("b/*.parquet").repartition(10, hash_by="id")
            c = sp.partial_sql("select * from {0} join {1} on a.id = b.id", a, b)
        """

        plan = SqlEngineNode(self._ctx, tuple(input.plan for input in inputs), query, **kwargs)
        recompute = any(input.need_recompute for input in inputs)
        return DataFrame(self, plan, recompute=recompute)

    def wait(self, *dfs: DataFrame):
        """
        Wait for all DataFrames to be computed.

        Example
        -------
        This can be used to wait for multiple outputs from a pipeline:

        .. code-block::

            df = sp.read_parquet("input/*.parquet")
            output1 = df.write_parquet("output1")
            output2 = df.map("col1, col2").write_parquet("output2")
            sp.wait(output1, output2)
        """
        ray.get([task.run_on_ray() for df in dfs for task in df._get_or_create_tasks()])

    def graph(self) -> Digraph:
        """
        Get the DataFrame graph.
        """
        dot = Digraph(comment="SmallPond")
        for node in self._nodes:
            dot.node(str(node.id), repr(node))
            for dep in node.input_deps:
                dot.edge(str(dep.id), str(node.id))
        return dot

    def shutdown(self):
        """
        Shutdown the session.
        """
        # prevent shutdown from being called multiple times
        if hasattr(self, "_shutdown_called"):
            return
        self._shutdown_called = True

        # log status
        finished = self._all_tasks_finished()
        with open(self._runtime_ctx.job_status_path, "a") as fout:
            status = "success" if finished else "failure"
            fout.write(f"{status}@{datetime.now():%Y-%m-%d-%H-%M-%S}\n")

        # clean up runtime directories if success
        if finished:
            logger.info("all tasks are finished, cleaning up")
            self._runtime_ctx.cleanup(remove_output_root=self.config.remove_output_root)
        else:
            logger.warning("tasks are not finished!")

        super().shutdown()

    def _summarize_task(self) -> Tuple[int, int]:
        """
        Return the total number of tasks and the number of tasks that are finished.
        """
        dataset_refs = [task._dataset_ref for tasks in self._node_to_tasks.values() for task in tasks if task._dataset_ref is not None]
        ready_tasks, _ = ray.wait(dataset_refs, num_returns=len(dataset_refs), timeout=0, fetch_local=False)
        return len(dataset_refs), len(ready_tasks)

    def _all_tasks_finished(self) -> bool:
        """
        Check if all tasks are finished.
        """
        dataset_refs = [task._dataset_ref for tasks in self._node_to_tasks.values() for task in tasks]
        try:
            ray.get(dataset_refs, timeout=0)
        except Exception:
            # GetTimeoutError is raised if any task is not finished
            # RuntimeError is raised if any task failed
            return False
        return True


class DataFrame:
    """
    A distributed data collection. It represents a 2 dimensional table of rows and columns.

    Internally, it's a wrapper around a `Node` and a `Session` required for execution.
    """

    def __init__(self, session: Session, plan: Node, recompute: bool = False, use_cache: bool = True):
        self.session = session
        self.plan = plan
        self.optimized_plan: Optional[Node] = None
        self.need_recompute = recompute
        """Whether to recompute the data regardless of whether it's already computed."""
        self._use_cache = use_cache
        """Whether to use caching for this DataFrame's computations."""
        self._compute_lock = Lock()
        """Lock to ensure thread-safe access to optimized_plan during _compute()."""

        session._nodes.append(plan)

    def __str__(self) -> str:
        return repr(self.plan)

    def _get_or_create_tasks(self) -> List[Task]:
        """
        Get or create tasks to compute the data.
        """
        # optimize the plan
        if self.optimized_plan is None:
            logger.info(f"optimizing\n{LogicalPlan(self.session._ctx, self.plan)}")
            self.optimized_plan = Optimizer(exclude_nodes=set(self.session._node_to_tasks.keys())).visit(self.plan)
            logger.info(f"optimized\n{LogicalPlan(self.session._ctx, self.optimized_plan)}")
        # return the tasks if already created
        if tasks := self.session._node_to_tasks.get(self.optimized_plan):
            return tasks

        # remove all completed task files if recompute is needed
        if self.need_recompute:
            remove_path(
                os.path.join(
                    self.session._runtime_ctx.completed_task_dir,
                    str(self.optimized_plan.id),
                )
            )
            logger.info(f"cleared all results of {self.optimized_plan!r}")

        # create tasks for the optimized plan
        planner = Planner(self.session._runtime_ctx)
        # let planner update self.session._node_to_tasks
        planner.node_to_tasks = self.session._node_to_tasks
        return planner.visit(self.optimized_plan)

    def is_computed(self) -> bool:
        """
        Check if the data is ready on disk.
        """
        if tasks := self.session._node_to_tasks.get(self.plan):
            _, unready_tasks = ray.wait(tasks, timeout=0)
            return len(unready_tasks) == 0
        return False

    def compute(self) -> None:
        """
        Compute the data.

        This operation will trigger execution of the lazy transformations performed on this DataFrame.
        """
        self._compute()

    def _compute(self, use_cache: Optional[bool] = None) -> List[DataSet]:
        """
        Compute the data and return the datasets.

        This method is thread-safe. Multiple threads calling _compute() on the same
        DataFrame will be serialized to avoid race conditions when optimizing the plan.

        Parameters
        ----------
        use_cache : bool, optional
            Whether to use caching for this computation.
            If None, uses the DataFrame's default cache setting.
            If True, will check the cache first and store results in cache.
            If False, will bypass the cache entirely.
        """
        # Determine if we should use cache
        should_use_cache = use_cache if use_cache is not None else self._use_cache

        # Use lock to ensure thread-safe access to optimized_plan
        with self._compute_lock:
            # Ensure the plan is optimized before checking cache
            if self.optimized_plan is None:
                # This will be set by _get_or_create_tasks, but we need it for cache key
                logger.info(f"optimizing\n{LogicalPlan(self.session._ctx, self.plan)}")
                self.optimized_plan = Optimizer(exclude_nodes=set(self.session._node_to_tasks.keys())).visit(self.plan)
                logger.info(f"optimized\n{LogicalPlan(self.session._ctx, self.optimized_plan)}")

            # Check cache first (unless recompute is needed or cache is disabled)
            if should_use_cache and not self.need_recompute:
                cached_result = self.session._cache.get(self.optimized_plan)
                if cached_result is not None:
                    return cached_result

            # Compute the data
            for retry_count in range(3):
                try:
                    result = ray.get([task.run_on_ray() for task in self._get_or_create_tasks()])

                    # Store in cache if caching is enabled
                    if should_use_cache:
                        self.session._cache.put(self.optimized_plan, result)

                    return result
                except ray.exceptions.RuntimeEnvSetupError as e:
                    # XXX: Ray may raise this error when a worker is interrupted.
                    #      ```
                    #      ray.exceptions.RuntimeEnvSetupError: Failed to set up runtime environment.
                    #      Failed to create runtime env for job 01000000, status = IOError:
                    #      on_read bad version, maybe there are some network problems, will fail the request.
                    #      ```
                    #      This is a bug of Ray and has been fixed in Ray 2.24: <https://github.com/ray-project/ray/pull/45513>
                    #      However, since Ray dropped support for Python 3.8 since 2.11, we can not upgrade Ray.
                    #      So we catch this error and retry by ourselves.
                    logger.error(f"found ray RuntimeEnvSetupError, retrying...\n{e}")
                    time.sleep(10 << retry_count)
            raise RuntimeError("Failed to compute data after 3 retries")

    # operations

    def recompute(self) -> DataFrame:
        """
        Always recompute the data regardless of whether it's already computed.

        This method also clears any cached result for this DataFrame to free memory,
        since the cached result will no longer be valid after recomputation.

        Examples
        --------
        Modify the code as follows and rerun:

        .. code-block:: diff

            - df = input.select('a')
            + df = input.select('b').recompute()

        The result of `input` can be reused.
        """
        # Clear the cached result if it exists, since we're going to recompute
        if self.optimized_plan is not None:
            cleared = self.session._cache.invalidate(self.optimized_plan)
            if cleared > 0:
                logger.debug(f"Cleared cached result for {self.optimized_plan!r} due to recompute()")

        self.need_recompute = True
        return self

    def no_cache(self) -> DataFrame:
        """
        Disable caching for this DataFrame's computations.

        When caching is disabled, results will always be computed fresh and
        will not be stored in or retrieved from the cache.

        Returns
        -------
        DataFrame
            Returns self for method chaining.

        Examples
        --------
        .. code-block::

            # Disable caching for a specific computation
            result = df.filter('x > 10').no_cache().count()

            # Chain with other operations
            df.map('a, b').no_cache().to_pandas()
        """
        self._use_cache = False
        return self

    def clear_cache(self) -> int:
        """
        Clear the cached result for this specific DataFrame.

        Returns
        -------
        int
            The number of cache entries removed (0 or 1).

        Examples
        --------
        .. code-block::

            df = sp.read_parquet("data/*.parquet").filter('x > 10')
            count1 = df.count()  # Computes and caches
            df.clear_cache()     # Clears cached result for this df
            count2 = df.count()  # Computes again
        """
        if self.optimized_plan is not None:
            return self.session._cache.invalidate(self.optimized_plan)
        return 0

    def repartition(
        self,
        npartitions: int,
        hash_by: Union[str, List[str], None] = None,
        by: Optional[str] = None,
        by_rows: bool = False,
        **kwargs,
    ) -> DataFrame:
        """
        Repartition the data into the given number of partitions.

        Parameters
        ----------
        npartitions
            The dataset would be split and distributed to `npartitions` partitions.
            If not specified, the number of partitions would be the default partition size of the context.
        hash_by, optional
            If specified, the dataset would be repartitioned by the hash of the given columns.
        by, optional
            If specified, the dataset would be repartitioned by the given column.
        by_rows, optional
            If specified, the dataset would be repartitioned by rows instead of by files.

        Examples
        --------
        .. code-block::

            df = df.repartition(10)                 # evenly distributed
            df = df.repartition(10, by_rows=True)   # evenly distributed by rows
            df = df.repartition(10, hash_by='host') # hash partitioned
            df = df.repartition(10, by='bucket')    # partitioned by column
        """
        if by is not None:
            assert hash_by is None, "cannot specify both by and hash_by"
            plan = ShuffleNode(
                self.session._ctx,
                (self.plan,),
                npartitions,
                data_partition_column=by,
                **kwargs,
            )
        elif hash_by is not None:
            hash_columns = [hash_by] if isinstance(hash_by, str) else hash_by
            plan = HashPartitionNode(self.session._ctx, (self.plan,), npartitions, hash_columns, **kwargs)
        else:
            plan = EvenlyDistributedPartitionNode(
                self.session._ctx,
                (self.plan,),
                npartitions,
                partition_by_rows=by_rows,
                **kwargs,
            )
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def random_shuffle(self, **kwargs) -> DataFrame:
        """
        Randomly shuffle all rows globally.
        """

        repartition = HashPartitionNode(
            self.session._ctx,
            (self.plan,),
            self.plan.num_partitions,
            random_shuffle=True,
            **kwargs,
        )
        plan = SqlEngineNode(
            self.session._ctx,
            (repartition,),
            r"select * from {0} order by random()",
            **kwargs,
        )
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def partial_sort(self, by: Union[str, List[str]], **kwargs) -> DataFrame:
        """
        Sort rows by the given columns in each partition.

        Parameters
        ----------
        by
            A column or a list of columns to sort by.

        Examples
        --------
        .. code-block::

            df = df.partial_sort(by='a')
            df = df.partial_sort(by=['a', 'b desc'])
        """

        by = [by] if isinstance(by, str) else by
        plan = SqlEngineNode(
            self.session._ctx,
            (self.plan,),
            f"select * from {{0}} order by {', '.join(by)}",
            **kwargs,
        )
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def filter(self, sql_or_func: Union[str, Callable[[Dict[str, Any]], bool]], **kwargs) -> DataFrame:
        """
        Filter out rows that don't satisfy the given predicate.

        Parameters
        ----------
        sql_or_func
            A SQL expression or a predicate function.
            For functions, it should take a dictionary of columns as input and returns a boolean.
            SQL expression is preferred as it's more efficient.

        Examples
        --------
        .. code-block::

            df = df.filter('a > 1')
            df = df.filter(lambda r: r['a'] > 1)
        """
        if isinstance(sql := sql_or_func, str):
            plan = SqlEngineNode(
                self.session._ctx,
                (self.plan,),
                f"select * from {{0}} where ({sql})",
                **kwargs,
            )
        elif isinstance(func := sql_or_func, Callable):

            def process_func(_runtime_ctx, tables: List[arrow.Table]) -> arrow.Table:
                table = tables[0]
                return table.filter([func(row) for row in table.to_pylist()])

            plan = ArrowBatchNode(self.session._ctx, (self.plan,), process_func=process_func, **kwargs)
        else:
            raise ValueError("condition must be a SQL expression or a predicate function")
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def map(
        self,
        sql_or_func: Union[str, Callable[[Dict[str, Any]], Dict[str, Any]]],
        *,
        schema: Optional[arrow.Schema] = None,
        **kwargs,
    ) -> DataFrame:
        """
        Apply a function to each row.

        Parameters
        ----------
        sql_or_func
            A SQL expression or a function to apply to each row.
            For functions, it should take a dictionary of columns as input and returns a dictionary of columns.
            SQL expression is preferred as it's more efficient.
        schema, optional
            The schema of the output DataFrame.
            If not passed, will be inferred from the first row of the mapping values.
        udfs, optional
            A list of user defined functions to be referenced in the SQL expression.

        Examples
        --------
        .. code-block::

            df = df.map('a, b')
            df = df.map('a + b as c')
            df = df.map(lambda row: {'c': row['a'] + row['b']})


        Use user-defined functions in SQL expression:

        .. code-block::

            @udf(params=[UDFType.INT, UDFType.INT], return_type=UDFType.INT)
            def gcd(a: int, b: int) -> int:
                while b:
                    a, b = b, a % b
                return a
            # load python udf
            df = df.map('gcd(a, b)', udfs=[gcd])

            # load udf from duckdb extension
            df = df.map('gcd(a, b)', udfs=['path/to/udf.duckdb_extension'])

        """
        if isinstance(sql := sql_or_func, str):
            plan = SqlEngineNode(self.session._ctx, (self.plan,), f"select {sql} from {{0}}", **kwargs)
        elif isinstance(func := sql_or_func, Callable):

            def process_func(_runtime_ctx, tables: List[arrow.Table]) -> arrow.Table:
                output_rows = [func(row) for row in tables[0].to_pylist()]
                return arrow.Table.from_pylist(output_rows, schema=schema)

            plan = ArrowBatchNode(self.session._ctx, (self.plan,), process_func=process_func, **kwargs)
        else:
            raise ValueError(f"must be a SQL expression or a function: {sql_or_func!r}")
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def flat_map(
        self,
        sql_or_func: Union[str, Callable[[Dict[str, Any]], List[Dict[str, Any]]]],
        *,
        schema: Optional[arrow.Schema] = None,
        **kwargs,
    ) -> DataFrame:
        """
        Apply a function to each row and flatten the result.

        Parameters
        ----------
        sql_or_func
            A SQL expression or a function to apply to each row.
            For functions, it should take a dictionary of columns as input and returns a list of dictionaries.
            SQL expression is preferred as it's more efficient.
        schema, optional
            The schema of the output DataFrame.
            If not passed, will be inferred from the first row of the mapping values.

        Examples
        --------
        .. code-block::

            df = df.flat_map('unnest(array[a, b]) as c')
            df = df.flat_map(lambda row: [{'c': row['a']}, {'c': row['b']}])
        """
        if isinstance(sql := sql_or_func, str):

            plan = SqlEngineNode(self.session._ctx, (self.plan,), f"select {sql} from {{0}}", **kwargs)
        elif isinstance(func := sql_or_func, Callable):

            def process_func(_runtime_ctx, tables: List[arrow.Table]) -> arrow.Table:
                output_rows = [item for row in tables[0].to_pylist() for item in func(row)]
                return arrow.Table.from_pylist(output_rows, schema=schema)

            plan = ArrowBatchNode(self.session._ctx, (self.plan,), process_func=process_func, **kwargs)
        else:
            raise ValueError(f"must be a SQL expression or a function: {sql_or_func!r}")
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def map_batches(
        self,
        func: Callable[[arrow.Table], arrow.Table],
        *,
        batch_size: int = 122880,
        **kwargs,
    ) -> DataFrame:
        """
        Apply the given function to batches of data.

        Parameters
        ----------
        func
            A function or a callable class to apply to each batch of data.
            It should take a `arrow.Table` as input and returns a `arrow.Table`.
        batch_size, optional
            The number of rows in each batch. Defaults to 122880.
        """

        def process_func(_runtime_ctx, tables: List[arrow.Table]) -> arrow.Table:
            return func(tables[0])

        plan = ArrowBatchNode(
            self.session._ctx,
            (self.plan,),
            process_func=process_func,
            streaming_batch_size=batch_size,
            **kwargs,
        )
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def limit(self, limit: int) -> DataFrame:
        """
        Limit the number of rows to the given number.

        Unlike `take`, this method doesn't trigger execution.
        """
        plan = LimitNode(self.session._ctx, self.plan, limit)
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def join(
        self,
        other: DataFrame,
        on: Union[str, List[str], None] = None,
        left_on: Union[str, List[str], None] = None,
        right_on: Union[str, List[str], None] = None,
        how: str = "inner",
        npartitions: Optional[int] = None,
        suffix: Tuple[str, str] = ("_left", "_right"),
    ) -> DataFrame:
        """
        Join this DataFrame with another DataFrame.

        This method automatically handles repartitioning both DataFrames by the join keys
        to ensure correct distributed join execution, and generates the appropriate SQL query.

        Parameters
        ----------
        other : DataFrame
            The right DataFrame to join with.
        on : str or List[str], optional
            Column name(s) to join on. Use this when the join columns have the same name
            in both DataFrames. Cannot be used together with `left_on`/`right_on`.
        left_on : str or List[str], optional
            Column name(s) from the left DataFrame (self) to join on.
            Must be used together with `right_on`.
        right_on : str or List[str], optional
            Column name(s) from the right DataFrame (other) to join on.
            Must be used together with `left_on`.
        how : str, default 'inner'
            Type of join to perform. Supported values:
            - 'inner': Inner join - only rows with matching keys in both DataFrames.
            - 'left': Left outer join - all rows from left DataFrame, matching rows from right.
            - 'right': Right outer join - all rows from right DataFrame, matching rows from left.
            - 'outer' or 'full': Full outer join - all rows from both DataFrames.
            - 'cross': Cross join - cartesian product of both DataFrames (no join keys needed).
            - 'semi': Semi join - rows from left DataFrame that have a match in right DataFrame.
            - 'anti': Anti join - rows from left DataFrame that have no match in right DataFrame.
        npartitions : int, optional
            Number of partitions to use for the join. If not specified, uses the maximum
            number of partitions from both DataFrames. This ensures that the larger DataFrame
            maintains its parallelism level, while the smaller DataFrame is repartitioned to
            match. Although this may create some empty or sparse partitions when DataFrames
            have very different sizes, it preserves the parallelism of the larger DataFrame
            and avoids the overhead of determining optimal partition counts dynamically.
        suffix : Tuple[str, str], default ('_left', '_right')
            Reserved for future use. Currently, when using the `on` parameter (same column
            names in both DataFrames), DuckDB's USING clause automatically deduplicates the
            join columns. When using `left_on`/`right_on` (different column names), all
            columns from both DataFrames are included in the result. If you need to handle
            overlapping non-join column names, use `map()` to rename columns before joining.

        Returns
        -------
        DataFrame
            A new DataFrame containing the joined data.

        Raises
        ------
        ValueError
            If join parameters are invalid (e.g., using both `on` and `left_on`/`right_on`,
            or specifying mismatched number of columns in `left_on` and `right_on`).

        Examples
        --------
        Inner join on a single column with the same name:

        .. code-block::

            result = df1.join(df2, on="id")

        Inner join on multiple columns:

        .. code-block::

            result = df1.join(df2, on=["id", "date"])

        Left join with different column names:

        .. code-block::

            result = df1.join(df2, left_on="user_id", right_on="id", how="left")

        Full outer join:

        .. code-block::

            result = df1.join(df2, on="id", how="outer")

        Cross join (cartesian product):

        .. code-block::

            result = df1.join(df2, how="cross")

        Notes
        -----
        - Both DataFrames are automatically repartitioned by the join keys using hash
          partitioning to ensure that matching rows end up in the same partition.
        - For cross joins, no repartitioning is performed since all combinations are needed.
        - The join is executed partition-by-partition using DuckDB SQL.
        """
        # Validate join type
        valid_join_types = {"inner", "left", "right", "outer", "full", "cross", "semi", "anti"}
        how_lower = how.lower()
        if how_lower not in valid_join_types:
            raise ValueError(
                f"Invalid join type '{how}'. Supported types are: {', '.join(sorted(valid_join_types))}"
            )

        # Normalize 'full' to 'outer' for SQL generation
        if how_lower == "full":
            how_lower = "outer"

        # Validate join key parameters
        if how_lower == "cross":
            # Cross join doesn't need join keys
            if on is not None or left_on is not None or right_on is not None:
                raise ValueError("Cross join does not accept join keys (on, left_on, right_on)")
            left_cols: List[str] = []
            right_cols: List[str] = []
        else:
            # Non-cross joins require join keys
            if on is not None:
                if left_on is not None or right_on is not None:
                    raise ValueError("Cannot specify both 'on' and 'left_on'/'right_on'")
                left_cols = [on] if isinstance(on, str) else list(on)
                right_cols = left_cols.copy()
            elif left_on is not None and right_on is not None:
                left_cols = [left_on] if isinstance(left_on, str) else list(left_on)
                right_cols = [right_on] if isinstance(right_on, str) else list(right_on)
                if len(left_cols) != len(right_cols):
                    raise ValueError(
                        f"left_on and right_on must have the same number of columns. "
                        f"Got {len(left_cols)} left columns and {len(right_cols)} right columns."
                    )
            elif left_on is not None or right_on is not None:
                raise ValueError("Must specify both 'left_on' and 'right_on', or use 'on' for same-named columns")
            else:
                raise ValueError(
                    f"Join keys required for '{how}' join. Use 'on' for same-named columns, "
                    "or 'left_on' and 'right_on' for different column names."
                )

        # Determine number of partitions.
        # We use the maximum partition count from both DataFrames to preserve parallelism
        # of the larger DataFrame. While this may result in some sparse partitions when
        # joining DataFrames of very different sizes, it avoids the complexity of dynamically
        # determining optimal partition counts based on data size, which would require
        # computing metadata before the join. Users can override this by specifying npartitions.
        if npartitions is None:
            npartitions = max(self.plan.num_partitions, other.plan.num_partitions)

        # Repartition both DataFrames by join keys (skip for cross join)
        if how_lower == "cross":
            left_df = self
            right_df = other
        else:
            left_df = self.repartition(npartitions, hash_by=left_cols)
            right_df = other.repartition(npartitions, hash_by=right_cols)

        # Build the SQL query
        sql = self._build_join_sql(how_lower, left_cols, right_cols, suffix)

        # Execute the join using partial_sql
        plan = SqlEngineNode(
            self.session._ctx,
            (left_df.plan, right_df.plan),
            sql,
        )
        recompute = self.need_recompute or other.need_recompute
        return DataFrame(self.session, plan, recompute=recompute, use_cache=self._use_cache)

    def _build_join_sql(
        self,
        how: str,
        left_cols: List[str],
        right_cols: List[str],
        suffix: Tuple[str, str],
    ) -> str:
        """
        Build the SQL query for the join operation.

        Parameters
        ----------
        how : str
            The join type (inner, left, right, outer, cross, semi, anti).
        left_cols : List[str]
            Column names from the left DataFrame to join on.
        right_cols : List[str]
            Column names from the right DataFrame to join on.
        suffix : Tuple[str, str]
            Suffixes for overlapping column names (reserved for future use).

        Returns
        -------
        str
            The SQL query string.
        """
        # Map join type to SQL syntax
        join_type_map = {
            "inner": "INNER JOIN",
            "left": "LEFT OUTER JOIN",
            "right": "RIGHT OUTER JOIN",
            "outer": "FULL OUTER JOIN",
            "cross": "CROSS JOIN",
            "semi": "SEMI JOIN",
            "anti": "ANTI JOIN",
        }
        join_clause = join_type_map[how]

        # Build the ON clause for non-cross joins
        if how == "cross":
            on_clause = ""
        else:
            conditions = []
            for left_col, right_col in zip(left_cols, right_cols):
                # Quote column names to handle special characters and reserved words
                left_quoted = f'"{left_col}"'
                right_quoted = f'"{right_col}"'
                conditions.append(f"__left__.{left_quoted} = __right__.{right_quoted}")
            on_clause = f" ON {' AND '.join(conditions)}"

        # For semi and anti joins, only select from left table
        if how in ("semi", "anti"):
            return f"SELECT __left__.* FROM {{0}} AS __left__ {join_clause} {{1}} AS __right__{on_clause}"

        if left_cols == right_cols:
            # Same column names in both DataFrames - use DuckDB's USING clause.
            # USING automatically deduplicates the join columns (includes them once in the result)
            # and is cleaner than manually excluding columns with complex SQL.
            using_cols = ", ".join(f'"{col}"' for col in left_cols)
            return f"SELECT * FROM {{0}} AS __left__ {join_clause} {{1}} AS __right__ USING ({using_cols})"
        else:
            # Different column names - select all columns from both tables.
            # Both join key columns will be included in the result (e.g., user_id and id).
            # If there are overlapping non-join column names, users should rename them
            # before joining using map() to avoid ambiguity.
            return f"SELECT __left__.*, __right__.* FROM {{0}} AS __left__ {join_clause} {{1}} AS __right__{on_clause}"

    def groupby_agg(
        self,
        by: Union[str, List[str]],
        aggs: Dict[str, Union[str, List[str]]],
        npartitions: Optional[int] = None,
    ) -> DataFrame:
        """
        Perform grouped aggregation on the DataFrame.

        This method groups the data by specified columns and computes aggregations
        on other columns. It automatically handles repartitioning by group keys and
        merges partial results across partitions correctly.

        Parameters
        ----------
        by : str or List[str]
            Column name(s) to group by.
        aggs : Dict[str, Union[str, List[str]]]
            A dictionary mapping column names to aggregation function(s).
            Keys are the column names to aggregate (must not overlap with `by` columns).
            Values can be a single aggregation function name (str) or a list of functions.
            Supported aggregation functions:
            - 'count': Count non-null values
            - 'sum': Sum of values
            - 'avg' or 'mean': Average of values
            - 'min': Minimum value
            - 'max': Maximum value
            - 'count_distinct': Count of distinct values. WARNING: This collects all
              distinct values in memory during the two-phase aggregation. Avoid using
              on columns with very high cardinality (millions of unique values) as it
              may cause memory issues.
        npartitions : int, optional
            Number of partitions to use for the partial aggregation phase. If not
            specified, uses the current number of partitions. This controls parallelism
            during the first phase where data is hash-partitioned by group keys and
            partial aggregates are computed per partition. The final aggregation phase
            always collects results into a single partition to combine partial results.

        Returns
        -------
        DataFrame
            A new DataFrame containing the grouped aggregation results.
            Output columns will be named as:
            - Group columns: original column names
            - Aggregated columns: '<column>_<agg_func>' (e.g., 'amount_sum', 'price_avg')

        Raises
        ------
        ValueError
            If `by` is empty, if an unsupported aggregation function is specified,
            or if any column appears in both `by` and `aggs`.

        Examples
        --------
        Single aggregation per column:

        .. code-block::

            result = df.groupby_agg(
                by='category',
                aggs={'amount': 'sum', 'price': 'avg'}
            )

        Multiple aggregations per column:

        .. code-block::

            result = df.groupby_agg(
                by=['region', 'category'],
                aggs={'amount': ['sum', 'count'], 'price': ['min', 'max', 'avg']}
            )

        Notes
        -----
        - This method uses a two-phase aggregation strategy:
          1. **Partial aggregation**: Data is hash-partitioned by group columns (using
             `npartitions`), and partial aggregates are computed in parallel per partition.
          2. **Final aggregation**: All partial results are collected into a single partition
             and combined to produce the final result.
        - For 'avg', partial sums and counts are computed separately, then combined as
          sum(partial_sums) / sum(partial_counts) to ensure correct weighted averages.
        - **Memory warning for 'count_distinct'**: This aggregation collects all distinct
          values per group into lists during partial aggregation. For columns with very
          high cardinality (millions of unique values), this can cause memory pressure.
          Consider using approximate distinct count techniques for such cases.
        """
        # Validate and normalize 'by' parameter
        if isinstance(by, str):
            group_cols = [by]
        else:
            group_cols = list(by)

        if not group_cols:
            raise ValueError("'by' parameter cannot be empty")

        # Validate and normalize 'aggs' parameter
        if not aggs:
            raise ValueError("'aggs' parameter cannot be empty")

        # Check for overlapping columns between group by and aggregation
        group_cols_set = set(group_cols)
        agg_cols_set = set(aggs.keys())
        overlapping_cols = group_cols_set & agg_cols_set
        if overlapping_cols:
            raise ValueError(
                f"Columns cannot be used for both grouping and aggregation: {sorted(overlapping_cols)}. "
                "A column should either be grouped by OR aggregated, not both."
            )

        supported_aggs = {'count', 'sum', 'avg', 'mean', 'min', 'max', 'count_distinct'}
        normalized_aggs: Dict[str, List[str]] = {}

        for col, funcs in aggs.items():
            if isinstance(funcs, str):
                funcs_list = [funcs]
            else:
                funcs_list = list(funcs)

            for func in funcs_list:
                func_lower = func.lower()
                if func_lower not in supported_aggs:
                    raise ValueError(
                        f"Unsupported aggregation function '{func}'. "
                        f"Supported functions are: {', '.join(sorted(supported_aggs))}"
                    )

            normalized_aggs[col] = funcs_list

        # Determine number of partitions
        if npartitions is None:
            npartitions = self.plan.num_partitions

        # Repartition by group columns to ensure all rows with same group keys
        # are in the same partition
        grouped_df = self.repartition(npartitions, hash_by=group_cols)

        # Build aggregation SQL and execute
        # We use a two-phase approach for correctness:
        # Phase 1: Compute partial aggregates per partition
        # Phase 2: Combine partial results

        partial_agg_sql, final_agg_sql = self._build_groupby_agg_sql(group_cols, normalized_aggs)

        # Phase 1: Partial aggregation per partition
        partial_plan = SqlEngineNode(
            self.session._ctx,
            (grouped_df.plan,),
            partial_agg_sql,
        )
        partial_df = DataFrame(self.session, partial_plan, recompute=self.need_recompute, use_cache=self._use_cache)

        # Phase 2: Collect all partial results into a single partition and compute final aggregates
        # Use repartition(1) to collect all partial results, then apply final aggregation
        collected_df = partial_df.repartition(1)

        final_plan = SqlEngineNode(
            self.session._ctx,
            (collected_df.plan,),
            final_agg_sql,
        )

        return DataFrame(self.session, final_plan, recompute=self.need_recompute, use_cache=self._use_cache)

    def _build_groupby_agg_sql(
        self,
        group_cols: List[str],
        aggs: Dict[str, List[str]],
    ) -> Tuple[str, str]:
        """
        Build SQL queries for two-phase grouped aggregation.

        Parameters
        ----------
        group_cols : List[str]
            Column names to group by.
        aggs : Dict[str, List[str]]
            Mapping of column names to list of aggregation functions.

        Returns
        -------
        Tuple[str, str]
            A tuple of (partial_sql, final_sql) for the two aggregation phases.
        """
        # Quote column names for SQL
        quoted_group_cols = [f'"{col}"' for col in group_cols]
        group_by_clause = ", ".join(quoted_group_cols)

        # Build partial and final aggregation expressions
        partial_select_parts = list(quoted_group_cols)
        final_select_parts = list(quoted_group_cols)

        for col, funcs in aggs.items():
            quoted_col = f'"{col}"'
            for func in funcs:
                func_lower = func.lower()
                output_name = f"{col}_{func_lower}"
                quoted_output = f'"{output_name}"'

                if func_lower in ('count',):
                    # COUNT: sum of partial counts
                    partial_select_parts.append(f"COUNT({quoted_col}) AS {quoted_output}")
                    final_select_parts.append(f"SUM({quoted_output}) AS {quoted_output}")

                elif func_lower in ('sum',):
                    # SUM: sum of partial sums
                    partial_select_parts.append(f"SUM({quoted_col}) AS {quoted_output}")
                    final_select_parts.append(f"SUM({quoted_output}) AS {quoted_output}")

                elif func_lower in ('min',):
                    # MIN: min of partial mins
                    partial_select_parts.append(f"MIN({quoted_col}) AS {quoted_output}")
                    final_select_parts.append(f"MIN({quoted_output}) AS {quoted_output}")

                elif func_lower in ('max',):
                    # MAX: max of partial maxes
                    partial_select_parts.append(f"MAX({quoted_col}) AS {quoted_output}")
                    final_select_parts.append(f"MAX({quoted_output}) AS {quoted_output}")

                elif func_lower in ('avg', 'mean'):
                    # AVG: requires sum and count, then sum(sum)/sum(count)
                    sum_name = f"{col}__avg_sum"
                    count_name = f"{col}__avg_count"
                    # Normalize output name to 'avg' even if user specified 'mean'
                    output_name = f"{col}_avg"
                    quoted_output = f'"{output_name}"'

                    partial_select_parts.append(f'SUM({quoted_col}) AS "{sum_name}"')
                    partial_select_parts.append(f'COUNT({quoted_col}) AS "{count_name}"')
                    final_select_parts.append(f'SUM("{sum_name}") / SUM("{count_name}") AS {quoted_output}')

                elif func_lower == 'count_distinct':
                    # COUNT_DISTINCT: collect distinct values per partition, then count distinct in final
                    # We use a list aggregation approach: collect values, then flatten and count distinct
                    list_name = f"{col}__distinct_list"
                    partial_select_parts.append(f'LIST(DISTINCT {quoted_col}) AS "{list_name}"')
                    final_select_parts.append(
                        f'(SELECT COUNT(DISTINCT val) FROM (SELECT UNNEST(LIST("{list_name}")) AS val)) AS {quoted_output}'
                    )

        partial_select = ", ".join(partial_select_parts)
        final_select = ", ".join(final_select_parts)

        partial_sql = f"SELECT {partial_select} FROM {{0}} GROUP BY {group_by_clause}"
        final_sql = f"SELECT {final_select} FROM {{0}} GROUP BY {group_by_clause}"

        return partial_sql, final_sql

    def sample(
        self,
        n: Optional[int] = None,
        fraction: Optional[float] = None,
        seed: Optional[int] = None,
    ) -> List[Dict[str, Any]]:
        """
        Return a random sample of rows from the DataFrame.

        This is useful for quickly inspecting a representative subset of the data
        without having to look at all rows. Users can specify either an exact number
        of rows or a fraction of total rows to sample.

        Parameters
        ----------
        n : int, optional
            The exact number of rows to sample. Must be a positive integer.
            Cannot be used together with `fraction`.
            If `n` is larger than the total number of rows in the DataFrame,
            all available rows will be returned (no error is raised).
        fraction : float, optional
            The fraction of rows to sample, between 0.0 and 1.0 (exclusive of 0, inclusive of 1).
            For example, 0.1 means 10% of rows. Cannot be used together with `n`.
        seed : int, optional
            Random seed for reproducible sampling. Can be used with either `n` or `fraction`.
            If not specified, sampling will be random each time.

        Returns
        -------
        List[Dict[str, Any]]
            A list of dictionaries, where each dictionary represents a row with column names as keys.

        Raises
        ------
        ValueError
            If neither `n` nor `fraction` is specified, or if both are specified.
            If `n` is not a positive integer.
            If `fraction` is not between 0 and 1.

        Examples
        --------
        Sample exactly 5 random rows:

        .. code-block::

            rows = df.sample(n=5)

        Sample 10% of the data:

        .. code-block::

            rows = df.sample(fraction=0.1)

        Sample with a fixed seed for reproducibility (using n):

        .. code-block::

            rows = df.sample(n=10, seed=42)

        Sample with a fixed seed for reproducibility (using fraction):

        .. code-block::

            rows = df.sample(fraction=0.2, seed=42)

        Notes
        -----
        This operation triggers execution of the lazy transformations performed on this DataFrame.
        For very large datasets, using `fraction` with a small value is more efficient than
        specifying a large `n`, as it can filter rows early in the pipeline.
        """
        # Validate parameters
        if n is None and fraction is None:
            raise ValueError("Must specify either 'n' (number of rows) or 'fraction' (proportion of rows)")
        if n is not None and fraction is not None:
            raise ValueError("Cannot specify both 'n' and 'fraction'. Please choose one.")
        if n is not None:
            if not isinstance(n, int) or n <= 0:
                raise ValueError(f"'n' must be a positive integer, got {n}")
        if fraction is not None:
            if not isinstance(fraction, (int, float)) or fraction <= 0 or fraction > 1:
                raise ValueError(
                    f"'fraction' must be a decimal number between 0 (exclusive) and 1 (inclusive), got {fraction}. "
                    "Examples: 0.1 for 10%, 0.25 for 25%, 0.5 for 50%, 1.0 for 100%."
                )

        # Build sample specification and method based on parameters
        if fraction is not None:
            sample_spec = f"{fraction * 100}%"
            sample_method = "bernoulli"
        else:
            sample_spec = f"{n} ROWS"
            sample_method = "reservoir"

        # Build SQL with optional seed for reproducibility
        seed_clause = f", repeatable({seed})" if seed is not None else ""
        sql = f"SELECT * FROM {{0}} USING SAMPLE {sample_spec} ({sample_method}{seed_clause})"

        plan = SqlEngineNode(self.session._ctx, (self.plan,), sql)
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache).take_all()

    def write_parquet(self, path: str) -> None:
        """
        Write data to a series of parquet files under the given path.

        This is a blocking operation. See :func:`write_parquet_lazy` for a non-blocking version.

        Examples
        --------
        .. code-block::

            df.write_parquet('output')
        """
        self.write_parquet_lazy(path).compute()

    def write_parquet_lazy(self, path: str) -> DataFrame:
        """
        Write data to a series of parquet files under the given path.

        This is a non-blocking operation. See :func:`write_parquet` for a blocking version.

        Examples
        --------
        .. code-block::

            o1 = df.write_parquet_lazy('output1')
            o2 = df.write_parquet_lazy('output2')
            sp.wait(o1, o2)
        """

        plan = DataSinkNode(self.session._ctx, (self.plan,), os.path.abspath(path), type="link_or_copy")
        return DataFrame(self.session, plan, recompute=self.need_recompute, use_cache=self._use_cache)

    # inspection

    def count(self) -> int:
        """
        Count the number of rows.

        If this dataframe consists of more than a read, or if the row count can't be determined from
        the metadata provided by the datasource, then this operation will trigger execution of the
        lazy transformations performed on this dataframe.
        """
        datasets = self._compute()
        # FIXME: don't use ThreadPoolExecutor because duckdb results will be mixed up
        return sum(dataset.num_rows for dataset in datasets)

    def take(self, limit: int) -> List[Dict[str, Any]]:
        """
        Return up to `limit` rows.

        This operation will trigger execution of the lazy transformations performed on this DataFrame.
        """
        if self.is_computed() or isinstance(self.plan, DataSourceNode):
            datasets = self._compute()
        else:
            datasets = self.limit(limit)._compute()
        rows = []
        for dataset in datasets:
            for batch in dataset.to_batch_reader():
                rows.extend(batch.to_pylist())
                if len(rows) >= limit:
                    return rows[:limit]
        return rows

    def take_all(self) -> List[Dict[str, Any]]:
        """
        Return all rows.

        This operation will trigger execution of the lazy transformations performed on this DataFrame.
        """
        datasets = self._compute()
        rows = []
        for dataset in datasets:
            for batch in dataset.to_batch_reader():
                rows.extend(batch.to_pylist())
        return rows

    def to_pandas(self) -> pd.DataFrame:
        """
        Convert to a pandas DataFrame.

        This operation will trigger execution of the lazy transformations performed on this DataFrame.
        """
        datasets = self._compute()
        with ThreadPoolExecutor() as pool:
            return pd.concat(pool.map(lambda dataset: dataset.to_pandas(), datasets))

    def to_arrow(self) -> arrow.Table:
        """
        Convert to an arrow Table.

        This operation will trigger execution of the lazy transformations performed on this DataFrame.
        """
        datasets = self._compute()
        with ThreadPoolExecutor() as pool:
            return arrow.concat_tables(pool.map(lambda dataset: dataset.to_arrow_table(), datasets))
